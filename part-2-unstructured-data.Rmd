---
title: "Part 2: Scraping UNstructured data"
author: Dan Turner (dturner@u.northwestern.edu)
---

```{r run this first}
library(rvest) # Web scraping
library(tidyverse) # Data wrangling
library(RCurl) # Download files from the internet

demo_mode <- TRUE # Skip the the code that takes more than a few minutes to run

```

# Part 2: **Scraping UNstructured data**

## 2.1 The structured-unstructured continuum
As you could tell from Part 1, small differences in how web pages are written can affect how well we are able to scrape them programatically. Websites that are user-driven, like Wikipedia, tend to be more variable than others, like Twitter.

Actually, Twitter and lots of other websites have a strict implicit data structure than can usually be accessed without scraping HTML, through an API. Constrast this with a table of values pasted in plaintext, which would necessitate lots of string handling functions. Speaking of handling problems...

## 2.2 How would you improve w.infobox()?

```{r}
# load the infoboxes
infoboxes <- read_rds("geo_content_infoboxes.rds") # load the infoboxe data

```

## 2.3 Scraping IMDB movie ratings from text

In the interest of scraping websites other than Wikipedia, let's scrape IMDB for the rating of each movie we can find. Are films set in California more enjoyable than films set in Minnesota? Probably!

There are many ways for web scraping to fail. Maybe you can't find a link that your script is supposed to follow, or maybe a link you found is dead (Error 404). You will see there are many levels of error testing in the following code chunk that address these problems. Sometimes I didn't know there would be a problem until I had scraped a few hundred pages!

In the next code chunk, I scan each film's Wikipedia page to look for a link to its page in the Internet Movie Database (IMDB). From IMDB, we will take the film's rating, which is stored in text, not a nicely structured table.

*In the next code chunk, we add a new column to our dataset for the IMDB rating.*

```{r Scrape IMDB ratings, message=FALSE, warning=FALSE}
# Reduce the dimensions
infoboxes <- subset(infoboxes, select = c(1:24, 31, 36, 42, 53) ) # only the cols that I want

# Scrape the Wikipedia pages for IMDB links and ratings
# Notice all of the error handling that I had to incorporate! There are many ways for web scraping to fail.

for(i in 1:nrow(infoboxes)) {  
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  rel_url <- infoboxes$links[i] # grab the relative url
  
  full_url <- paste0("https://www.wikipedia.org", rel_url) # build the full url
  
  ext_urls <- try(read_html( full_url ) %>%
    html_nodes("a.external.text") %>% # all external links on the page
    html_attr('href'))
  
  if("try-error" %in% class(ext_urls) ){ next } # if no external URLs
  
  imdb_url <- first(ext_urls[str_detect(ext_urls, "imdb")]) # first one we find (not ideal)
  
  if(any("try-error" %in% class(imdb_url),
         is.na(imdb_url),
         url.exists( imdb_url ) == FALSE)){ next } # if no IMDB link in external URLs
  
  print(paste0("#", i, ": IMDB page found for ", infoboxes$titles[i])) # status message

  # extract the rating using regex
  imdb_rating <- read_html( imdb_url ) %>%
    html_nodes("div.ratingValue") %>% # the rating box, out of 10
    html_text() %>% #"\n3.2/10"
    str_replace_all("[/10\n]" , "") %>% # delete the denominator and new line, if they are there
    parse_number() # turn the string into a number

  if(is_empty(imdb_rating) == FALSE) {
        infoboxes[i, "imdb_rating"] <- imdb_rating # if we find a rating, write it
    } 

  if(i > 249) { break } # you get the idea
  
  rm(rel_url, full_url, ext_urls, imdb_url, imdb_rating)
}
rm(i)

# Save the results, so you don't have to run the loop yourself
# saveRDS(infoboxes, "infoboxes_rated.rds")

```

## 2.4 Uisng tidyverse code to quickly structure the output

*How does my hypothesis bear out? Let's group by state and see what the average rating is across the states.*

```{r}

infoboxes <- read_rds("infoboxes_rated.rds")

rating.by.state <- infoboxes %>%
  filter(!is.na(imdb_rating)) %>% # drop rows where we don't have a rating
  mutate("state" = str_match(parent_title, ".*\\in\\s(.*)")[, 2] ) %>% # add a "state" col
  group_by(state) %>%
  summarise(mean_rating = mean(imdb_rating) )

# peek
head(rating.by.state)
```

Now that you have seen what it takes to extract a single number from a webpage, let's see how we can extract informtion from nothing but text on the web.


## 2.5 Scrape and interpret text (and text-like) data

Say that we want to look for patterns in the scripts for these films. You can imagine testing a hypothesis about whether some linguistic features predict higher ratings, or whether budget or setting leads to higher ratings or earnings -- quetsions like these.

*Let's extract some information from the scripts of these films from The Internet Movie Script Database (IMSDb).*

First we need to find our films in their database, but luckily they build their film lists using the alphabet. For example, for "A" films, the URL is:

```
https://www.imsdb.com/alphabetical/A
```

This is really common, and we can programmatically change the URL to get the next letter of the alphabet if we need to. Alternatively, some websites (like IMSDb) have a list of all pages:

```
https://www.imsdb.com/all%20scripts/
```

*Let's scrape this list and see if we can match any films in infoboxes.*

```{r}

# Let's start by getting every link and its text
imsdb_titles <- read_html( "https://www.imsdb.com/all%20scripts/" ) %>%
    html_nodes("td  p  a") %>% # all external links on the page
    html_text() 

imsdb_links <- read_html( "https://www.imsdb.com/all%20scripts/" ) %>%
    html_nodes("td  p  a") %>% # all external links on the page
    html_attr('href')

infoboxes_scripts <- data.frame("titles" = imsdb_titles, "script" = imsdb_links ) # simple is good

# Now let's intersect our script database with our film database by title
infoboxes_scripts <- merge.data.frame(infoboxes_scripts, infoboxes, by = "titles")

rm(imsdb_links, imsdb_titles, infoboxes) # cleanup
```

Now we have links to the pages on the script database website in our main dataframe. Let's write a function that scrapes the scripts.


## 2.6 Scrape the scripts for movies we found on Wikipedia 
The links we scraped don't go straight to the scripts; there is an intervening page, for example:

```
https://www.imsdb.com/Movie%20Scripts/Time%20Machine,%20The%20Script.html
```

...includes the link. Note that spaces are filled with "%20". This is because spaces are not legal in URLs. Why_not_use_underscores? My theory is that underscores look too much like an underline and thus would be less visible.

```
<a href="/scripts/Time-Machine,-The.html">Read "Time Machine, The" Script</a>
```

Using the Inspector view, I extracted this rule just like we did before:

```
body > table:nth-child(3) > tbody > tr > td:nth-child(3) > table.script-details > tbody > tr:nth-child(2) > td:nth-child(2) > a:nth-child(32)
```

*Imagine if the rule was just "a".* It would scrape every link on the webpage. We want something like that, but only for the big table. To get every link in that table, I used the very simple rule "td > a", so every link in the table (<td> is a component of a table).


```{r}

# convenience function for getting the script link from a imsdb.com page
script_link <- function( full_url ){
  
  if( url.exists( full_url ) == FALSE ){ return(NA) } # return NA if 404
  
  the_link <- read_html( full_url ) %>%
    html_nodes("td > a") %>% # all external links on the page
    html_attr('href') %>% # get the link
    na.omit() %>% # remove NA's
    last() # alternatively, you can use a:last-child in your rule
  
  if(!str_detect(the_link, "scripts")){ # then we did not get the right URL
    the_link <- NA
  }
  return(the_link)
}#/script_link()

# convenience function to scrape the script of a script page
script_read <- function(rel_url){
  
  the_full_url <- paste0("https://www.imsdb.com", rel_url) # build the full url
  
  the_full_url <- gsub(" ", "%20", the_full_url) # replace spaces with %20, like the website does
  
  the_script_link <- script_link( the_full_url )
  
  if( is.na(the_script_link) ){ return(NA) } # return NA if 404
  
  the_script_text <- read_html( paste0("https://www.imsdb.com", the_script_link )) %>% # needs full url
    html_nodes(".scrtext") %>% # all of the text
    html_text() %>%
    str_replace_all("[\r\n]" , "") %>% # delete the returns and new lines
    str_squish() # delete extra spaces
  
  the_script_wordcount <- sapply(strsplit(the_script_text, " "), length) # rough token count
  
  print(paste0("Scraped ", the_script_wordcount, " words from ", rel_url)) # status message
  
  return(the_script_text)
}

# For example, let's get the script of Big Fish,
big_fish <- script_read( infoboxes_scripts$script[which(infoboxes_scripts$titles == "Big Fish")] )

word(big_fish, 17, 98) # take a peek at the beginning

```

*Now that we have the text, we scrape for practically anything, like named entities, topics, dialog, and so on.*

# Wrapping up
Now there are a few more examples out there of how to scrape the web with R and rvest! I hope it was helpful to practice some web scraping with different types of data and websites. As you have seen, scraping the web is easy, but failure-prone. It requires lots of branching logic, error handling, and patience. 



# Bonus: A quick text analysis of the scripts
Let's do a quick analysis of this data, since we have a nice script-scraping function to implement.

One way we can explore the scripts is by their type-token frequnecy ratio. This is the ratio of unique words to total words, so a higher number means more of the words are unique, and therefore the text may be more complex.

```{r}
# Let's get the type/token frequnecy for each film in the list
for(i in 1:nrow(infoboxes_scripts)){
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  # download the script for the film
  script_rel_url <- paste0( infoboxes_scripts$script[i] ) # get the rel link from the dataframe
  the_script_text <- script_read( script_rel_url )
  
  if(any(is.na(the_script_text),
         is_empty(the_script_text))){ next }  # catch 404s end reading errors
  
  # calculate the ratio for the script
  the_script_text <- str_to_lower(the_script_text) # for a more accurate count
  
  token <- length( unlist( strsplit(the_script_text, " ") ) )
  type <- length( unique( unlist( strsplit(the_script_text, " ") ) ) )
  ratio <- type / token
  
  infoboxes_scripts[i, "type-token-ratio"] <- ratio
  
}
rm(i)

```

Now let's compare each states' films by popularity and complexity, as a toy analysis.

```{r}

scripts.by.state <- infoboxes_scripts %>%
  filter(!is.na(imdb_rating)) %>% # drop rows where we don't have a rating
  mutate("state" = str_match(parent_title, ".*\\in\\s(.*)")[, 2] ) %>% # add a "state" col
  group_by(state) %>%
  summarise(popularity = mean(imdb_rating),
            complexity = mean(type-token-ratio))

# peek
head(scripts.by.state)

```

-Do you see any interesting patterns?
-What are other questions might this data be useful for?

You made it to the end! I hope this tutorial was useful to you.