---
title: "Part 1.5: Web scraping practice"
author: Dan Turner (dturner@u.northwestern.edu)
---

```{r run this first}
library(rvest) # Web scraping
library(tidyverse) # Data wrangling
library(RCurl) # Download files from the internet

```

# Part 1.5: **Web scraping practice**

Below you will find some web scraping challenges. Try out the functions I covered in part 1.

## Challenge 1
Modify the rule below to list the titles of all the blog posts on the first page found at the url:

```{r Challenge 1}

url <- "https://forum.thegradcafe.com/blogs/"

# rule <- "#ipsLayout_mainArea > section > div:nth-child(8) > article:nth-child(1) > div.cBlog_grid_item__body.ipsPad > div:nth-child(1) > h2 > span > a"

rule <- "#ipsLayout_mainArea > section > div > article:nth-child(2) > div.cBlog_grid_item__body.ipsPad > div:nth-child(2) > div > div.ipsPhotoPanel.ipsPhotoPanel_notPhone.ipsPhotoPanel_tiny > div > p:nth-child(1) > a"

read_html(url) %>% 
  html_nodes(rule) %>%
  html_text()

```

```{r Challenge 1 answer}

url <- "https://forum.thegradcafe.com/blogs/"
rule <- "#ipsLayout_mainArea > section > div > article > div.cBlog_grid_item__body.ipsPad > div > h2 > span > a"

read_html(url) %>% 
  html_nodes(rule) %>%
  html_text()

```

## Challenge 2
Modify the rule below to make a dataframe consisting of the titles, links, author, and date. The author and date will require you to use Inspector view to build and test two more rules.

```{r Challenge 2}

url <- "https://forum.thegradcafe.com/blogs/"
rule <- "#ipsLayout_mainArea > section > div > article > div.cBlog_grid_item__body.ipsPad > div > h2 > span > a"

author_rule <- ""

date_rule <- ""

titles <- read_html(url) %>% 
  html_nodes(rule) %>%
  html_text()

links <- read_html(url) %>% 
  html_nodes(rule) %>%
  html_attr('href')

# authors <- 

# dates <- 

# df <- data.frame(titles, links, authors, dates, stringsAsFactors = FALSE)

```


```{r Challenge 2 answer}

url <- "https://forum.thegradcafe.com/blogs/"
rule <- "#ipsLayout_mainArea > section > div > article > div.cBlog_grid_item__body.ipsPad > div > h2 > span > a"

author_rule <- "#ipsLayout_mainArea > section > div > article > div.cBlog_grid_item__body.ipsPad > div > div > div.ipsPhotoPanel.ipsPhotoPanel_notPhone.ipsPhotoPanel_tiny > div > p > a"

date_rule <- "#ipsLayout_mainArea > section > div > article > div.cBlog_grid_item__body.ipsPad > div > div > div.ipsPhotoPanel.ipsPhotoPanel_notPhone.ipsPhotoPanel_tiny > div > p > time"

titles <- read_html(url) %>% 
  html_nodes(rule) %>%
  html_text()

links <- read_html(url) %>% 
  html_nodes(rule) %>%
  html_attr('href')

authors <- read_html(url) %>% 
  html_nodes(author_rule) %>%
  html_text()

dates <- read_html(url) %>% 
  html_nodes(date_rule) %>%
  html_text()

df <- data.frame(titles, links, authors, dates, stringsAsFactors = FALSE)

```

## Challenge 3
Now that we can extract data from one page, let's get every page. Write a function that lists every page of blog posts. There are many ways to do this one, so do not feel like you are required to write a recursive solution. For ideas, take a look at the url structure:

```
<a href="https://forum.thegradcafe.com/blogs/?page=2" data-page="2">2</a>
```

```{r Challenge 3}
# for sample answers, see the repo

## YOUR CODE HERE

```


```{r Challenge 3 answer}

# just a sample answer!

# maximal laziness. let's find out how many pages there are...
page_rule <- "li.ipsPagination_last a"

# the url of the last page has its page number in it
page.count <- read_html(url) %>% 
  html_nodes(page_rule) %>%
  html_attr('href')

# scanning for the number
page.count <- as.numeric(gsub("[^\\d]+", "", page.count, perl=TRUE))

# I think this is the most simple solution
all_the_links <- paste0("https://forum.thegradcafe.com/blogs/?page=", 1:page.count)

```
