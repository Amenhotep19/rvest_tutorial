---
title: "Part 1: Scraping structured data"
author: Dan Turner (dturner@u.northwestern.edu)
---

```{r run this first}
library(rvest) # Web scraping
library(tidyverse) # Data wrangling
library(RCurl) # Download files from the internet

demo_mode <- TRUE # Skip the the code that takes more than a few minutes to run

```

# Part 1: **Scraping structured data**

## 1.1 Inspecting the page source

Before we can scrape websites we need to understand how they are put together. Code for websites is extremely variable in quality and organization, and so the first step to extracting data from a webpage is exploring how the data is encoded/stored/displayed.

We will use a *web inspector tool* to do this. Follow the instructions depending on your browser of choice, in case you have not used such a tool before.

If you would rather not use a browser, try using SelectorGadget: https://selectorgadget.com


*How to open Inspector view*
(Tested on Chrome, Safari, Edge and Firefox)

Right-click and element on the web page you want to inspect and click 'Inspect'/'Inspect element'. This should open a new pane or window that shows the HTML code corresponding to the element you selected.


*Using Inspector view*
You can browse the code and see how it was interpreted into the visual layout that you see, you can see the CSS style properties for every element, and you can even test changes to any of the website's code in realtime.

Even if you never use it, it's good to know such a tool exists, since scammers use this method to extort money from people over the internet every day. 

We will use CSS and HTML tags to crawl the webpages and scrape only the data we want (as much as possible).


*HTML versus CSS*
A quick note about how webpages work. Most websites use two major file types, HTML and CSS. HTML (hyper text markup language) is a data file that contains code representing text, links to pictures, tables of data, and everything else. CSS (cascading style sheets) contains code that browsers use to visually style the HTML. In the old days, HTML code would have tags for bold <b> and italics <em> and so on, but not anymore. For a long time, it has been best practices to do all styling using CSS classes.

We can scrape elements of websites using their HTML (hierarchically grouped) OR CSS (stylistically grouped). Adanced users might get use out of xpath as well, but that is beyond the scope of this workshop (sorry).

## 1.2 Principles of scraping
Because the code for websites is often poorly written (sorry, webdevs), I want to offer some guidelines to help decide when and how to develop a web scraping solution for your project.

Rule 1. Don't scrape what you can download
Rule 2. Don't scrape what you cannot easilly clean
Rule 3. Convert data into native data types ASAP (from strings)


## 1.3 Scraping Wikipedia
For our first example, we will scrape some lists from Wikipedia.

Let's compare a list of films set in Minnesota (A) to a list of films actually shot in Minnesota (B). I want to use these lists to answer the simple question, do films shot in Minnesota tend to be set in Minnesota? (Someday I want to make a map of North America showing how likely it is for people living in an area to have seen a film depicting that area -- just for fun.)

Link A. https://en.wikipedia.org/wiki/Category:Films_set_in_Minnesota
Link B. https://en.wikipedia.org/wiki/Category:Films_shot_in_Minnesota

Movie titles in these lists are represented in bullet pointed lists organized into alphabetical categories. The code for the first category ("0-9") looks like this:

```
  <div class="mw-category-group"><h3>0â€“9</h3>
  <ul><li><a href="/wiki/20/20:_In_an_Instant" title="20/20: In an Instant">20/20: In an Instant</a></li>
  <li><a href="/wiki/360_(film)" title="360 (film)">360 (film)</a></li></ul></div>
```

Here is a quick breakdown of these tags:
`div` means 'division', which is used to apply the "mw-category-group" CSS class to this chunk of HTML
`ul` means 'unorderd list' = bullet point list
`li` means 'list item' = individual bullet point
`a` means 'anchor link' = normal hyperlink

For my simple research question, we only need the names of the films, but if we wanted to know more about these films later (say, their release date or budget), we might want the link to their Wikipedia page. Check out one of the movie's pages to get a sense for the data potential: https://en.wikipedia.org/wiki/Purple_Rain_(film)

*With this in mind, let's scrape Link A and Link B to get the titles of the films (the text in the <a> tag) and their links (the <a> tag's 'href').*


## 1.4 Scraping the movie titles and links

```{r}
# Download the html of the two links into R variables
films_set_html <- read_html( "https://en.wikipedia.org/wiki/Category:Films_set_in_Minnesota" )
films_shot_html <- read_html( "https://en.wikipedia.org/wiki/Category:Films_shot_in_Minnesota" )

# Peek
word( html_text( films_shot_html ), 100, 200)
```

If the packages are installed correctly, you should see the list punctuated by new line tags (`\n`),

*Next we will scrape the data we want using by using the information from Inspector View to write some inclusion/exclusion rules.*

Usually you can simply right-click an example of the information you want in Inspector View and use that to build your rule. When you right-click the line, select 'Copy' and 'Selector Path'. On the page of films set in Minnesota, this gives us:

```
#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a
```

*If we paste that into the `html_nodes()` function, it returns the first title of the first film on the page ('The Adventures of Rocky and Bullwinkle').*

```{r}
films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a') %>%
  html_text() # this extracts text from within HTML tags
```

But we want every film, not just the first one. If you look at the rule, you will see two references to `nth-child(1)`, which is a newer way to specify CSS styles based on parent-child relationships and order (with ':').

*Delete it to include all of the films in all of the categories:*

```{r}
films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_text() # this extracts text from within HTML tags
```

Now let's finish the job, by storing the links and titles in R variables.

```{r}
# Titles, same as above
films_set_titles <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 

# The rule works equally well for the other link, too
films_set_links <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")

# Join the titles and links as a data frame
films_set_mn <- data.frame("title" = films_set_titles, "link" = films_set_links)

# Peek
head(films_set_mn)

# Cleanup
rm(films_set_html, films_set_titles, films_set_links)
```

## 1.5 Code it up
*Write the code to scrape the titles and links for the 'films_shot' list.*

```{r}

# Titles, same as above
films_shot_titles <- films_shot_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 

# The rule works equally well for the other link, too
films_shot_links <- films_shot_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")

# Join the titles and links as a data frame
films_shot_mn <- data.frame("title" = films_shot_titles, "link" = films_shot_links)

# Peek
head(films_shot_mn)

# Cleanup
rm(films_shot_html, films_shot_titles, films_shot_links)
```

## 1.6 Answering our question

Do films shot in Minnesota tend to be set in Minnesota?

```{r}
# Films shot in MN but NOT set in MN
length(setdiff(films_shot_mn$title, films_set_mn$title)) / nrow(films_shot_mn)

# Films set in MN but NOT shot in MN
length(setdiff(films_set_mn$title, films_shot_mn$title)) / nrow(films_shot_mn)

```

A little more than half of the films shot in MN are not set in MN.


## 1.7 Diving a little deeper

Let's get a little more local and get the titles and links for films set and shot in Chicago.

*But there is a problem.* There are many more films set and shot in Chicago than in Minnesota, and Wikipedia only lists 200 items per list per page. See for yourself:

```{r}
# same as before
films_set_chicago <- read_html( "https://en.wikipedia.org/wiki/Category:Films_set_in_Chicago" ) %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_text()

length(films_set_chicago)
```

## 1.8 A problem with the simple solution
The list only has *200* items in it, but according to the link we are scraping, the full list is about twice that size. If we were browsing Wikipedia, we could click "next page" and see how the list continues, but that's not how we're reading it.

One solution to this issue is to scrape every page and remove any duplicates we find. This type of problem (that the next page can have a next page, which can have a next page) is elegantly handled using *recursive programming*.

```{r}
# function to scrape links and names
w.scrape <- function(full_url, rule){

  # get titles
  the_titles <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_text()
  
  # get links
  the_links <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_attr('href')
  
  # as a dataframe
  df <- data.frame("titles" = the_titles, "links" = the_links,
                   stringsAsFactors = FALSE) 
  
  return ( df)
}

# return the urls of the next pages
w.tree <- function(rel_url){
  root = "https://en.wikipedia.org"
  
  full_url <- paste0(root, rel_url)

  # see if there's a next page link
  to.continue <- read_html( full_url ) %>% 
             html_node("#mw-pages > a:last-child") %>% # the 'next page' link is the last link in this div
             html_text() %>%
             all_equal("next page")
  
  # if so, get the link
  if(to.continue == TRUE){
    
    next.page <- read_html( full_url ) %>% 
      html_node("#mw-pages > a:last-child") %>% # the 'next page' link is the last link in this div
      html_attr('href')

    w.tree(next.page) # recurse
    
    return(next.page)
  }
}

# convenience function to make a list of urls and their text from wikipedia category pages
w.list <- function(rel_url, rule){
  
  root = "https://en.wikipedia.org"

  to.scrape <- c(rel_url, w.tree( rel_url ) ) # not tested beyond 2 pages
  
  output <- data.frame() # container
  
  for(page in to.scrape){
    output <- rbind( w.scrape( paste0(root, page), rule ), output )
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
  }
  
  return(unique(output)) # return unique rows
  
}

```

The reason why we're using loops is so we can pause between itterations. This is important if you want to be a friendly scraper.

```{r message=FALSE, warning=FALSE}

child_rule = "#mw-pages li  a"

# let's see how it works
films_set_chicago <- w.list( rel_url = "/wiki/Category:Films_set_in_Chicago",
                               rule = child_rule )

```

I get 392 films, which means we have just under 2 pages of items. 

We could apply this function to other categories one at a time, but we could also scrape film lists for other regions.


## 1.9 Thinking a little bigger

Minnesota versus Chicago is not a fair comparison. Minnesota is a whole state (5.64M) and Chicago (2.7M) is only a city.

We could scrape every city or every state, or both, using the same basic methods as we employed for the Chicago list and the URLS below:

```{r message=FALSE, warning=FALSE}

# let's get all of the movies from these links
rel_link_list <- c( "/wiki/Category:Films_set_in_the_United_States_by_state",
                    "/wiki/Category:Films_shot_in_the_United_States_by_state")

# using the Inspector tool on 'Films set in Akron, Ohio' I copied the selector path
# I also had to delete the 'child' selectors, as I did before
parent_rule <- '#mw-subcategories li a'
child_rule = "#mw-pages li a"

# loop all the geographical area links to get all the list page links

# scrape all the geo categories
for(link in rel_link_list){
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  geo_links <- w.list( rel_url = link, rule = parent_rule )
  
  geo_content <- data.frame(matrix(ncol = 5, nrow = 0))
  
  # for each geo category scrape the links
  for(i in 1:nrow(geo_links)) {
    
    temp <- w.list( rel_url = paste0( geo_links$links[i] ) ,
                  rule = child_rule )
    
    if(nrow(temp) == 0){ 
      
      next # if our rule fails, skip this link
      
    } else {
      
          temp$parent_title <- geo_links$titles[i]
          temp$parent_link <- geo_links$links[i]
    
          geo_content <- rbind(geo_content, temp)
        
      }

    rm(temp)
    
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
    
  }#inner
}#outer

rm(link, i, geo_links, rel_link_list) # cleanup

# saving this so you can load it without running it
# saveRDS(geo_content, "geo_content.rds") # 6062 films set in various states

```

This just shows how you can easilly scale-up some simple scraping script into something with more of an appetite.

Now let's scrape some details about each of the films.

## 1.10 Scraping HTML Tables

If we use the Inspector view on one of the film's pages, we can see that there is an HTML table that floats on the right side with some useful data, like budget and personelle. Let's extract that for each of the films we found while scraping the category lists.

For example, the film Airborne (https://en.wikipedia.org/wiki/Airborne_(1993_film) which is set in Cinncinatti, Ohio:


```{r}

  # get titles
  read_html( "https://en.wikipedia.org/wiki/Airborne_(1993_film)" ) %>%
    html_nodes( "#mw-content-text > div > table.infobox.vevent" ) %>% # directly copied from Inspector view
    html_table(header = TRUE) %>% # html_table() interprets HTML tables into R dataframes
    first()

```

Let's quickly extract this data for all of our films. I say "quickly" in terms of the code writing. This actually took a couple of hours to run, which is why I saved the results for everyone to load and explore. 
In fact, I had to run the script a couple of times until it could find all of the pages it was looking for. You can see the logic here.

```
if( !is.na(geo_content$`Release date`[i]) ){ next } # skip rows we've probably crawled
```

Because almost every film seemed to have an infobox, and most infoboxes had the release date, this seemed like the best way to tell whether we should skip a row or not.

```{r message=FALSE, warning=FALSE}

# grab the data
geo_content <- read_rds("geo_content.rds")

# a function to scrape the infobox for films on Wikipedia
w.infobox <- function( full_link ){
  
  # check the url to make sure it isn't 404'd using RCurl
  if( url.exists( full_link ) == FALSE){
    return("404")
  } else {
    
    rm(temp_html, temp_data) # clean slate
    
    # download the html and scrape for the infobox
    temp_html <- read_html( full_link ) %>%
      html_node( "#mw-content-text > div > table.infobox.vevent" ) # directly copied from Inspector view (node returns first match; nodes returns a list)
    
    if(is_empty(temp_html)) { return("no infobox") } # catch when the rule does not find an infobox

    # if we get to this point, we have data in the infobox! let's save it as a dataframe by interpreting the HTML table
    temp_data <- temp_html %>%
      html_table(header = TRUE, fill = TRUE ) # html_table() interprets HTML tables into R dataframes
    }

  return(temp_data)
  
}#/w.infobox

# Now we will fetch every infobox and combine it itteratively with our main dataframe

# We could apply this function and get our data faster, but I prefer to loop because I don't want to spike the traffic
for(i in 1:nrow(geo_content)){
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  if(all( !is.null(geo_content$`Release date`[i]),       # don't evaluate release date until we have one
           !is.na(geo_content$`Release date`[i]) )){ next }# skip rows we've probably crawled
  
  print(paste0("#", i, ": ", geo_content$titles[i] )) # status message
  
  # build the full url to pass to w.infobox()
  rel_link <- geo_content$links[i]
  full_link <- paste0("https://www.wikipedia.org", rel_link)
  
  # scrape the infobox
  infobox <- w.infobox( full_link )
  
  if(infobox == "404" | infobox == "no infobox" ) { next } # skip if we can't find the link
  
  # setup the output
  df_vars <- as.list(infobox[,1]) # vars
  df_vals <- as.list(infobox[,2]) # vals
  
  # now that we have the output, let's add this to geo_content
  infobox <- data.frame(matrix(ncol = length(df_vars), nrow = 1))
  infobox[1,] <- df_vals
  colnames(infobox) <- df_vars
  
  # merge -- there is no reason this has to be a loop; it should really be vectorized
  for (col in df_vars){
    
    if(nchar(col) == 0) { next } # skip blank columns
    
    col <- unlist(col)
    
    geo_content[i, col] <- infobox[1, col] # add all the information we can find
    
    # Sys.sleep(0.5) # pause 1/2 second before scraping the next page
    
  }#innerfor

  rm(df_vars, df_vals, infobox, col)

}#outerfor
rm(i)

# save the data so everyone can just load it
saveRDS(geo_content, "geo_content_infoboxes.rds")

```

In the next code chunk, we load the result (it took some time to run) and see how we did.

```{r}
# load the progress
geo_content_infoboxes.2 <- read_rds("geo_content_infoboxes.rds")

# what headers did we extract from the infoboxes?
colnames(geo_content_infoboxes)
```

Some columns make sense, but others do not. This is because some movie pages have infoboxes that are formatted a little differently. We will talk about how to handle this ubiquitous problem on the other side of the break. Also, I will ask what you would do to improve our Wikipedia infobox scraping script. I have a few ideas, but I want to hear yours!


## 1.11 Time to test your skills!
Before going onto Part 2, we will take a break from my examples for you to practice your skills. In Part 2, I'll show some more tips and tricks for scraping text and other unstructured data.

```{r final cleanup, message=FALSE, warning=FALSE}

# removing everything made in this file -- we will load what we need in the next part
rm(films_set_chicago, films_set_mn, films_shot_mn, 
   geo_content, geo_content_infoboxes, child_rule, 
   parent_rule, demo_mode, w.infobox, w.list,
   w.scrape, w.tree)

```

